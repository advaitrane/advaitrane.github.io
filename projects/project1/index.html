<!DOCTYPE html>
<html>

<head>
    
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer">

<title>
    Human-in-the-loop RL for Behaviour Change(2020) - Advait Rane
</title>









<link rel="stylesheet" href="/css/main.min.38d14cb9c68f9c625ef2dd5992d5959f19c9bcea52929ec71ae385f260c5241b.css" integrity="sha256-ONFMucaPnGJe8t1ZktWVnxnJvOpSkp7HGuOF8mDFJBs=" crossorigin="anonymous" media="screen">




  






<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Didact+Gothic">

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Human-in-the-loop RL for Behaviour Change(2020)"/>
<meta name="twitter:description" content="Human-in-the-loop, Interpretable, Sample-efficient RL for Behaviour Change Interventions (PAL)&nbsp;¶ Supervisor - Mina Khan
Members - Advait Rane, P Srivatsa
I am working on the Project PAL in the Fluid Interfaces Group at MIT Media Lab for my undergraduate thesis.
Habit formation can be supported with computer-generated interventions. However, for these interventions to be helpful they should be personalised and context-specific. For a healthy interaction with the user the system should learn to adapt to the user fast, and the user should be able to understand and control its behaviour."/>

<meta property="og:title" content="Human-in-the-loop RL for Behaviour Change(2020)" />
<meta property="og:description" content="Human-in-the-loop, Interpretable, Sample-efficient RL for Behaviour Change Interventions (PAL)&nbsp;¶ Supervisor - Mina Khan
Members - Advait Rane, P Srivatsa
I am working on the Project PAL in the Fluid Interfaces Group at MIT Media Lab for my undergraduate thesis.
Habit formation can be supported with computer-generated interventions. However, for these interventions to be helpful they should be personalised and context-specific. For a healthy interaction with the user the system should learn to adapt to the user fast, and the user should be able to understand and control its behaviour." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://advaitrane.github.io/projects/project1/" />
<meta property="article:published_time" content="2020-11-27T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-11-27T00:00:00+00:00" />


    

    
    
    
    <title>
        
        Human-in-the-loop RL for Behaviour Change(2020)
        
    </title>
</head>

<body>
    
    
    <header class="wrap flex-container">
        <h1>Human-in-the-loop RL for Behaviour Change(2020)</h1>
    </header>
    
    <main class="wrap">
        
        <article role="article" class="flex-container">
<h3 id="human-in-the-loop-interpretable-sample-efficient-rl-for-behaviour-change-interventions-pal" class="anchor-link"><a href="#human-in-the-loop-interpretable-sample-efficient-rl-for-behaviour-change-interventions-pal">Human-in-the-loop, Interpretable, Sample-efficient RL for Behaviour Change Interventions (PAL)<span class="pilcrow">&nbsp;¶</span></a></h3>
<p>Supervisor - Mina Khan</p>
<p>Members - Advait Rane, P Srivatsa</p>
<p>I am working on the Project PAL in the Fluid Interfaces Group at MIT Media Lab for my undergraduate thesis.</p>
<p>Habit formation can be supported with computer-generated interventions. However, for these interventions to be helpful they should be personalised and context-specific. For a healthy interaction with the user the system should learn to adapt to the user fast, and the user should be able to understand and control its behaviour. Thus, sample-efficiency and interpretability are key to the system.</p>
<p>As a part of this thesis, we relied on Reinforcement Learning (RL) to learn the most beneficial interventions. We built an RL environment and designed datasets to test the above desiderata in an RL model. We evaluated the use of different sequence models to learn user behaviour patterns to make learning sample-efficient. We further evaluated the performance of different RL algorithms to determine the best choice in terms of sample efficiency and integration of human guidance. To test the models, we used computer-usage data to learn to give calming interventions during computer usage. We also built and deployed a Chrome extension which gives calming interventions based on the user’s preferences while browsing the internet.</p>
<p>The image below depicts a visualisation of the computer usage data.
<p style="text-align:center;">
    <img src="/projects/project_images/states-history-js.jpeg" alt="computer usage states"  />
</p>
</p>
<p>For details, you can refer to my <a href="/projects/project_links/AdvaitRane_TSReport.pdf">thesis</a>.</p>
</article>
        

        
        
        <nav role="navigation" class="flex-container bottom-menu">
            
<hr />
<p>


    

    
        
            <a href="/about">about</a>
        
    
    
        
            &#183; 
            <a href="/projects">portfolio</a>
        
            &#183; 
            <a href="/basketball">basketball</a>
        
            &#183; 
            <a href="/blog">blog</a>
        
            &#183; 
            <a href="/">home</a>
        
    
    &#183; 
    <a href="/">
        
    </a>

</p>
        </nav>
        
        
    </main>
    
    <footer class="flex-container footer">
</footer>
    
    
</body>

</html>